
@article{cheston_trade-offs_2024,
	title = {Trade-offs in Coordination Strategies for Duet Jazz Performances Subject to Network Delay and Jitter},
	volume = {42},
	issn = {0730-7829},
	url = {https://osf.io/z8c7w},
	doi = {10.1525/mp.2024.42.1.48},
	pages = {48--72},
	number = {1},
	journaltitle = {Music Perception},
	author = {Cheston, Huw and Cross, Ian and Harrison, Peter M. C.},
	urldate = {2023-11-13},
	date = {2024},
	langid = {english},
	doi = {10.31234/osf.io/z8c7w},
	file = {Cheston et al. - 2023 - Trade-offs in Coordination Strategies for Networke.pdf:C\:\\Users\\huwch\\Zotero\\storage\\5L7WLRZ2\\Cheston et al. - 2023 - Trade-offs in Coordination Strategies for Networke.pdf:application/pdf},
        keywords = {paper}
}

@article{cheston_jazz_2024,
	title = {Jazz trio database: Automated annotation of jazz piano trio recordings processed using audio source separation},
	volume = {7},
	url = {https://doi.org/10.5334/tismir.186},
	doi = {10.5334/tismir.186},
	pages = {144--158},
	number = {1},
	journaltitle = {Transactions of the International Society for Music Information Retrieval},
	shortjournal = {Trans. Int. Soc. Music Inf. Retr.},
	author = {Cheston, Huw and Schlichting, Joshua L and Cross, Ian and Harrison, Peter M C},
	date = {2024},
	file = {Cheston et al. - 2024 - Jazz Trio Database Automated Annotation of Jazz P.pdf:C\:\\Users\\huwch\\Zotero\\storage\\H4IX6V56\\Cheston et al. - 2024 - Jazz Trio Database Automated Annotation of Jazz P.pdf:application/pdf},
        keywords = {paper}
}

@article{cheston_rhythmic_2024,
	title = {Rhythmic qualities of jazz improvisation predict performer identity and style in source-separated audio recordings},
	volume = {11},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.240920},
	doi = {10.1098/rsos.240920},
	abstract = {Great musicians have a unique style and, with training, humans can learn to distinguish between these styles. What differences between performers enable us to make such judgements? We investigate this question by building a machine learning model that predicts performer identity from data extracted automatically from an audio recording. Such a model could be trained on all kinds of musical features, but here we focus specifically on rhythm, which (unlike harmony, melody and timbre) is relevant for any musical instrument. We demonstrate that a supervised learning model trained solely on rhythmic features extracted from 300 recordings of 10 jazz pianists correctly identified the performer in 59\% of cases, six times better than chance. The most important features related to a performer’s ‘feel’ (ensemble synchronization) and ‘complexity’ (information density). Further analysis revealed two clusters of performers, with those in the same cluster sharing similar rhythmic traits, and that the rhythmic style of each musician changed relatively little over the duration of their career. Our findings highlight the possibility that artificial intelligence can perform performer identification tasks normally reserved for experts. Links to each recording and the corresponding predictions are available
              on an interactive map
              to support future work in stylometry.},
	pages = {240920},
	number = {11},
	journaltitle = {Royal Society Open Science},
	shortjournal = {R. Soc. Open Sci.},
	author = {Cheston, Huw and Schlichting, Joshua L. and Cross, Ian and Harrison, Peter M. C.},
	urldate = {2024-11-13},
	date = {2024-11},
	langid = {english},
	file = {Cheston et al. - 2024 - Rhythmic qualities of jazz improvisation predict p.pdf:C\:\\Users\\huwch\\Zotero\\storage\\R498KDS6\\Cheston et al. - 2024 - Rhythmic qualities of jazz improvisation predict p.pdf:application/pdf},
        keywords = {paper}
}


@article{cheston_deconstructing_2025,
	title = {Deconstructing Jazz Piano Style Using Machine Learning},
	url = {http://arxiv.org/abs/2504.05009},
	doi = {10.48550/arXiv.2504.05009},
	abstract = {Artistic style has been studied for centuries, and recent advances in machine learning create new possibilities for understanding it computationally. However, ensuring that machine-learning models produce insights aligned with the interests of practitioners and critics remains a significant challenge. Here, we focus on musical style, which benefits from a rich theoretical and mathematical analysis tradition. We train a variety of supervised-learning models to identify 20 iconic jazz musicians across a carefully curated dataset of 84 hours of recordings, and interpret their decision-making processes. Our models include a novel multi-input architecture that enables four musical domains (melody, harmony, rhythm, and dynamics) to be analysed separately. These models enable us to address fundamental questions in music theory and also advance the state-of-the-art in music performer identification (94\% accuracy across 20 classes). We release open-source implementations of our models and an accompanying web application for exploring musical styles.},
	journaltitle = {{arXiv}},
	author = {Cheston, Huw and Bance, Reuben and Harrison, Peter M. C.},
	urldate = {2025-04-23},
	date = {2025-04-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2504.05009 [cs]},
	keywords = {preprint},
	file = {Cheston et al. - 2025 - Deconstructing Jazz Piano Style Using Machine Lear.pdf:C\:\\Users\\huwch\\Zotero\\storage\\U3N5Q95B\\Cheston et al. - 2025 - Deconstructing Jazz Piano Style Using Machine Lear.pdf:application/pdf},
}

@article{cheston_automatic_2025,
	title = {Automatic Identification of Samples in Hip-Hop Music via Multi-Loss Training and an Artificial Dataset},
	url = {http://arxiv.org/abs/2502.06364},
	doi = {10.48550/arXiv.2502.06364},
	abstract = {Sampling, the practice of reusing recorded music or sounds from another source in a new work, is common in popular music genres like hip-hop and rap. Numerous services have emerged that allow users to identify connections between samples and the songs that incorporate them, with the goal of enhancing music discovery. Designing a system that can perform the same task automatically is challenging, as samples are commonly altered with audio effects like pitch- and time-stretching and may only be seconds long. Progress on this task has been minimal and is further blocked by the limited availability of training data. Here, we show that a convolutional neural network trained on an artificial dataset can identify real-world samples in commercial hip-hop music. We extract vocal, harmonic, and percussive elements from several databases of non-commercial music recordings using audio source separation, and train the model to fingerprint a subset of these elements in transformed versions of the original audio. We optimize the model using a joint classification and metric learning loss and show that it achieves 13\% greater precision on real-world instances of sampling than a fingerprinting system using acoustic landmarks, and that it can recognize samples that have been both pitch shifted and time stretched. We also show that, for half of the commercial music recordings we tested, our model is capable of locating the position of a sample to within five seconds.},
	journaltitle = {{arXiv}},
	author = {Cheston, Huw and Balen, Jan Van and Durand, Simon},
	urldate = {2025-02-17},
	date = {2025-02-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2502.06364 [cs]},
	keywords = {preprint},
	file = {Cheston et al. - 2025 - Automatic Identification of Samples in Hip-Hop Mus.pdf:C\:\\Users\\huwch\\Zotero\\storage\\ZT4UA24P\\Cheston et al. - 2025 - Automatic Identification of Samples in Hip-Hop Mus.pdf:application/pdf},
	% number = {{arXiv}:2502.06364},
}

@inproceedings{cheston2024DMRN,
  author    = {Cheston, H. and Bance, R. and Harrison, P.},
  title     = {Characterizing Jazz Improvisation Style Through Explainable Performer Identification Models},
  year      = {2024},
  booktitle = {DMRN+19: Digital Music Research Network Workshop},
  address   = {Queen Mary University of London, UK},
  url       = {https://www.qmul.ac.uk/dmrn/media/dmrn/DMRN-19-proceedings--January.pdf},
  keywords   = {proceedings}
}

@inproceedings{cheston2023automated,
  author    = {Cheston, H. and Cross, I. and Harrison, P.},
  title     = {An Automated Pipeline for Characterizing Timing in Jazz Trios},
  year      = {2023},
  booktitle = {DMRN+18: Digital Music Research Network Workshop},
  address   = {Queen Mary University of London, UK},
  url       = {https://www.qmul.ac.uk/dmrn/media/dmrn/DMRN-18-Proceedings.pdf},
  keywords   = {proceedings}
}

inproceedings{cheston2023modelling,
  author    = {Cheston, H. and Cross, I. and Harrison, P.},
  title     = {Modelling Coordination Strategies in Improvised Musical Performances by Skilled Jazz Groups},
  year      = {2023},
  booktitle = {16th International Conference of Students of Systematic Musicology (SysMus23)},
  address   = {Sheffield University, UK},
  url       = {https://drive.google.com/file/d/14F5Xe8qfxfWpMW6pGuIl8fcaIYbmFDtt/view},
  keywords   = {proceedings}
}

inproceedings{cheston2023coordination,
  author    = {Cheston, H. and Cross, I. and Harrison, P.},
  title     = {Coordination Strategies in Networked Jazz Performances},
  year      = {2023},
  booktitle = {17th International Conference on Music Perception and Cognition (ICMPC)},
  address   = {Nihon University, Tokyo, Japan},
  url       = {https://icmpc17.com/proceedings-in-zip/ICMPC17-APSCOM7-e-Proceedings.zip/},
  keywords   = {proceedings}
}

inproceedings{cheston2022effects,
  author    = {Cheston, H. and Cross, I. and Harrison, P.},
  title     = {The Effects of Variable Latency Timings and Jitter on Networked Musical Performances},
  year      = {2022},
  booktitle = {15th International Conference of Students of Systematic Musicology (SysMus22)},
  address   = {University of Ghent, Belgium},
  url       = {http://hdl.handle.net/1854/LU-01GVD6WBCVPGRAHMNYR1CEXF57},
  keywords   = {proceedings}
}

inproceedings{cheston2022measuring,
  author    = {Cheston, H. and Cross, I. and Harrison, P.},
  title     = {Measuring the Effects of Variable Latency Timings on Networked Jazz Performances},
  year      = {2022},
  booktitle = {SEMPRE 50th Anniversary Conference},
  address   = {Senate House, University of London, UK},
  url       = {https://drive.google.com/file/d/1P72Orm1gqSI4_gOah3Ueb9fdCpriVY_M/view},
  keywords   = {proceedings}
}

inproceedings{cheston2022turning,
  author    = {Cheston, H.},
  title     = {“Turning the Beat Around”: Time, Temporality, and Participation in the Jazz Solo Break},
  year      = {2022},
  booktitle = {13th Conference on Interdisciplinary Musicology: 'Participation'},
  address   = {University of Edinburgh, UK},
  url       = {http://journals.ed.ac.uk/CIM22-Proceedings},
  keywords   = {proceedings}
}

